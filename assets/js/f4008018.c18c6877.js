"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[434],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>f});var r=n(7294);function s(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){s(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,r,s=function(e,t){if(null==e)return{};var n,r,s={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(s[n]=e[n]);return s}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(s[n]=e[n])}return s}var p=r.createContext({}),l=function(e){var t=r.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},c=function(e){var t=l(e.components);return r.createElement(p.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},k=r.forwardRef((function(e,t){var n=e.components,s=e.mdxType,i=e.originalType,p=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),u=l(n),k=s,f=u["".concat(p,".").concat(k)]||u[k]||d[k]||i;return n?r.createElement(f,a(a({ref:t},c),{},{components:n})):r.createElement(f,a({ref:t},c))}));function f(e,t){var n=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var i=n.length,a=new Array(i);a[0]=k;var o={};for(var p in t)hasOwnProperty.call(t,p)&&(o[p]=t[p]);o.originalType=e,o[u]="string"==typeof e?e:s,a[1]=o;for(var l=2;l<i;l++)a[l]=n[l];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}k.displayName="MDXCreateElement"},4551:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>l});var r=n(7462),s=(n(7294),n(3905));const i={sidebar_position:2},a="SparkSession in test",o={unversionedId:"testing/spark-session-in-test",id:"testing/spark-session-in-test",title:"SparkSession in test",description:"Spark needs a SparkSession to run the dataset jobs. ZIO Spark test provides helpers to",source:"@site/../docs/testing/spark-session-in-test.md",sourceDirName:"testing",slug:"/testing/spark-session-in-test",permalink:"/zio-spark/testing/spark-session-in-test",draft:!1,editUrl:"https://github.com/univalence/zio-spark/edit/master/docs/../docs/testing/spark-session-in-test.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Installing ZIO Spark Test",permalink:"/zio-spark/testing/installation"},next:{title:"Overview",permalink:"/zio-spark/experimental/overview"}},p={},l=[{value:"One file SparkSession",id:"one-file-sparksession",level:2},{value:"Multi files SparkSession",id:"multi-files-sparksession",level:2},{value:"Overriding SparkSession configuration",id:"overriding-sparksession-configuration",level:2}],c={toc:l},u="wrapper";function d(e){let{components:t,...n}=e;return(0,s.kt)(u,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"sparksession-in-test"},"SparkSession in test"),(0,s.kt)("p",null,"Spark needs a SparkSession to run the dataset jobs. ZIO Spark test provides helpers to\nget this SparkSession for free."),(0,s.kt)("h2",{id:"one-file-sparksession"},"One file SparkSession"),(0,s.kt)("p",null,"Any objects that implements ",(0,s.kt)("inlineCode",{parentName:"p"},"ZIOSparkSpecDefault")," trait is a runnable spark test.\nSo to start writing tests we need to extend ",(0,s.kt)("inlineCode",{parentName:"p"},"ZIOSparkSpecDefault"),", which requires a ",(0,s.kt)("inlineCode",{parentName:"p"},"Spec"),":"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-scala"},'import zio.test._\nimport zio.spark.test._\nimport zio.spark.sql.implicits._\n\nobject MySpecs extends ZIOSparkSpecDefault {\n  override def sparkSpec =\n    suite("ZIOSparkSpecDefault can run spark job without providing layer")(\n      test("It can run Dataset job") {\n        for {\n          df    <- Dataset(1, 2, 3)\n          count <- df.count\n        } yield assertTrue(count == 3L)\n      }\n    )\n}\n')),(0,s.kt)("admonition",{type:"info"},(0,s.kt)("p",{parentName:"admonition"},"It is exactly the same thing as ",(0,s.kt)("a",{parentName:"p",href:"https://zio.dev/reference/test/writing-our-first-test"},"ZIOSpecDefault"),".\nIt just provides a custom SparkSession by default.")),(0,s.kt)("h2",{id:"multi-files-sparksession"},"Multi files SparkSession"),(0,s.kt)("p",null,"The issue with the above example is that it will try to run many SparkSession if you have\ntests in different files. Sadly, it is not correct since it will try to start many spark\nclusters at the same time. Hopefully, you can bypass it using ",(0,s.kt)("inlineCode",{parentName:"p"},"SharedZIOSparkSpecDefault"),"."),(0,s.kt)("p",null,"It is the same kind of code, however it will ensure that all the jobs from the different files\nare finished before closing a unique SparkSession:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-scala"},'import zio.test._\nimport zio.spark.test._\nimport zio.spark.sql.implicits._\n\nobject MySpecs extends SharedZIOSparkSpecDefault {\n  override def spec =\n    suite("SharedZIOSparkSpecDefault can run shared spark job without providing layer")(\n      test("It can run Dataset job") {\n        for {\n          df    <- Dataset(1, 2, 3)\n          count <- df.count\n        } yield assertTrue(count == 3L)\n      }\n    )\n}\n')),(0,s.kt)("h2",{id:"overriding-sparksession-configuration"},"Overriding SparkSession configuration"),(0,s.kt)("p",null,"By default, here is the SparkSession configuration:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-scala"},'import zio.spark._\n\nval defaultSparkSession: SparkSession.Builder =\nSparkSession.builder\n  .master("local[*]")\n  .config("spark.sql.shuffle.partitions", 1)\n  .config("spark.ui.enabled", value = false)\n')),(0,s.kt)("p",null,"You can override it simply as follows:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-scala"},'import zio.test._\nimport zio.spark.test._\nimport zio.spark.sql.implicits._\n\nabstract class ZIOSparkSpec extends ZIOSparkSpecDefault {\n  override def ss: SparkSession.Builder = super.ss.config("", "")\n}\n')),(0,s.kt)("p",null,"And uses it the same way :"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-scala"},"import zio.test._\nimport zio.spark.test._\nimport zio.spark.sql.implicits._\n\nobject MySpecs extends ZIOSparkSpec {\n  override def spec = ???\n}\n")))}d.isMDXComponent=!0}}]);