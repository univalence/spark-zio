package zio.spark.sql

import zio.Task
import zio.spark.helper.Fixture._
import zio.test._

import scala.reflect.io.Directory

import java.io.File

object DataFrameWriterTest {
  val reader: DataFrameReader = SparkSession.read

  /** Deletes the folder generated by the test. */
  def deleteGeneratedFolder(path: String): Task[Unit] = Task(new Directory(new File(path)).deleteRecursively())

  def dataFrameWriterSaveSpec: Spec[SparkSession, TestFailure[Any], TestSuccess] = {
    final case class WriterTest(
        extension: String,
        readAgain: String => Spark[DataFrame],
        write:     String => DataFrame => Task[Unit]
    ) {

      def build: Spec[SparkSession, TestFailure[Any], TestSuccess] =
        test(s"DataFrameWriter can save a DataFrame to $extension") {
          val path: String = s"$resourcesPath/output.$extension"

          val pipeline = Pipeline.buildWithoutTransformation(read)(write(path))

          for {
            _      <- pipeline.run
            df     <- readAgain(path)
            output <- df.count
            _      <- deleteGeneratedFolder(path)
          } yield assertTrue(output == 4L)
        }
    }

    val tests =
      List(
        WriterTest(
          extension = "csv",
          readAgain = path => readCsv(path),
          write     = path => _.write.withHeader.csv(path)
        ),
        WriterTest(
          extension = "parquet",
          readAgain = path => SparkSession.read.parquet(path),
          write     = path => _.write.parquet(path)
        ),
        WriterTest(
          extension = "json",
          readAgain = path => SparkSession.read.json(path),
          write     = path => _.write.json(path)
        )
      )

    suite("DataFrameWriter Saving Formats")(tests.map(_.build): _*)
  }
}
